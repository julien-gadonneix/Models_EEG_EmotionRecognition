{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "wdir = Path.cwd().parent.absolute()\n",
    "sys.path.insert(1, str(wdir))\n",
    "\n",
    "from models.EEGModels import EEGNet, ShallowConvNet, DeepConvNet, EEGNet_ChanRed, CapsEEGNet\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:  50946\n"
     ]
    }
   ],
   "source": [
    "model = EEGNet(nb_classes = 2, Chans = 14, Samples = 128, dropoutRate = 0.1, kernLength = 16, F1 = 64, D = 8, F2 = 64, dropoutType = 'Dropout')\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total number of parameters: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:  53045\n"
     ]
    }
   ],
   "source": [
    "model_SEED = EEGNet_ChanRed(nb_classes = 3, Chans = 62, InnerChans=14, Samples = 200, dropoutRate = 0.1, kernLength = 25, F1 = 64, D = 8, F2 = 64, dropoutType = 'Dropout')\n",
    "\n",
    "total_params = sum(p.numel() for p in model_SEED.parameters())\n",
    "print(\"Total number of parameters: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters:  1218320\n"
     ]
    }
   ],
   "source": [
    "model_capseegnet = CapsEEGNet(nb_classes=3)\n",
    "\n",
    "total_params = sum(p.numel() for p in model_capseegnet.parameters())\n",
    "print(\"Total number of parameters: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 1, 200])\n",
      "torch.Size([1, 8, 32, 1, 96])\n",
      "torch.Size([1, 3072, 8])\n",
      "torch.Size([1, 3, 16, 1])\n",
      "torch.Size([1, 3, 16, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juliengadonneix/anaconda3/envs/intern3A/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1712608633180/work/aten/src/ATen/native/Convolution.cpp:1032.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 62, 200)\n",
    "out = model_capseegnet(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "0\n",
      "mps\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(os.cpu_count())\n",
    "print(torch.cuda.device_count()) \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(device)\n",
    "print(device.type == 'cuda')\n",
    "if device.type == 'cuda':\n",
    "    properties = torch.cuda.get_device_properties(device)\n",
    "    print(properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import plistlib\n",
    "\n",
    "def get_gpu_info():\n",
    "    \"\"\"Get information about the GPU on macOS.\"\"\"\n",
    "    # Run system_profiler command and capture the output\n",
    "    result = subprocess.run(['system_profiler', '-xml', 'SPDisplaysDataType'], capture_output=True)\n",
    "    if result.returncode == 0:\n",
    "        # Parse the XML output\n",
    "        display_info = plistlib.loads(result.stdout)\n",
    "        # Extract GPU information\n",
    "        gpu_count = len(display_info[0]['_items'])\n",
    "        return gpu_count\n",
    "    else:\n",
    "        print(\"Error:\", result.stderr)\n",
    "        return None\n",
    "\n",
    "gpu_count = get_gpu_info()\n",
    "if gpu_count is not None:\n",
    "    print(\"Number of GPUs:\", gpu_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Time:  0.7246112823486328\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# syncrocnize time with cpu, otherwise only time for oflaoding data to gpu would be measured\n",
    "torch.mps.synchronize()\n",
    "\n",
    "a = torch.ones(4000,4000, device=\"mps\")\n",
    "for _ in range(200):\n",
    "   a +=a\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print( \"GPU Time: \", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4993,  1.0841,  0.0559,  0.1679, -0.5550, -0.0980, -0.9082, -0.6369],\n",
      "        [ 0.5351,  1.4419,  0.1808,  1.9187,  1.6569, -0.5565,  0.3886, -0.2302],\n",
      "        [ 2.0292,  0.8219, -1.5713,  0.0808,  0.5523, -0.5211, -1.1286, -0.1915],\n",
      "        [ 0.2730,  2.3373,  0.3642,  0.5787, -1.8168, -1.2895, -1.5716, -1.8972],\n",
      "        [ 0.7142, -0.1486,  1.1180,  0.2327,  1.3839, -0.9730,  0.8136, -0.0598],\n",
      "        [ 1.4267,  0.5381,  1.3016, -1.2277,  0.0903,  0.4886,  0.5824, -0.6297],\n",
      "        [ 0.9264,  0.7906,  1.5842, -0.5572,  0.0211,  0.4384, -0.3296,  0.4292],\n",
      "        [-0.9650,  0.1832,  0.6145,  0.3053,  0.1119,  0.7097, -0.3226,  0.0037]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4993,  1.0841,  0.0559,  0.1679, -0.5550, -0.0980, -0.9082,\n",
       "          -0.6369],\n",
       "         [ 0.5351,  1.4419,  0.1808,  1.9187,  1.6569, -0.5565,  0.3886,\n",
       "          -0.2302]],\n",
       "\n",
       "        [[ 2.0292,  0.8219, -1.5713,  0.0808,  0.5523, -0.5211, -1.1286,\n",
       "          -0.1915],\n",
       "         [ 0.2730,  2.3373,  0.3642,  0.5787, -1.8168, -1.2895, -1.5716,\n",
       "          -1.8972]],\n",
       "\n",
       "        [[ 0.7142, -0.1486,  1.1180,  0.2327,  1.3839, -0.9730,  0.8136,\n",
       "          -0.0598],\n",
       "         [ 1.4267,  0.5381,  1.3016, -1.2277,  0.0903,  0.4886,  0.5824,\n",
       "          -0.6297]],\n",
       "\n",
       "        [[ 0.9264,  0.7906,  1.5842, -0.5572,  0.0211,  0.4384, -0.3296,\n",
       "           0.4292],\n",
       "         [-0.9650,  0.1832,  0.6145,  0.3053,  0.1119,  0.7097, -0.3226,\n",
       "           0.0037]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(8,8)\n",
    "print(t)\n",
    "torch.reshape(t, (-1, 2, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intern3A",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
